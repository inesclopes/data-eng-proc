{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/challenges/challenge_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# CHALLENGE 1\n",
        "##  Implement INGESTION process\n",
        "- Set up path in the \"lake\"\n",
        "  - !mkdir -p /content/lake/bronze\n",
        "\n",
        "- Read data from API https://api.carrismetropolitana.pt/\n",
        "  - Endpoints:\n",
        "    - vehicles\n",
        "    - lines\n",
        "    - municipalities\n",
        "  - Use StructFields to enforce schema\n",
        "\n",
        "- Transformations\n",
        "  - vehicles\n",
        "    - create \"date\" extracted from \"timestamp\" column (format: hh24miss)\n",
        "\n",
        "- Write data as PARQUET into the BRONZE layer (/content/lake/bronze)\n",
        "  - Partition \"vehicles\" by \"date\" column\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/bronze/vehicles\n",
        "    - lines - path: /content/lake/bronze/lines\n",
        "    - municipalities - path: /content/lake/bronze/municipalities\n",
        "  - Make sure there is only 1 single parquet created\n",
        "  - Use overwrite as write mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "c410e46c-4a50-43aa-926f-d0417c6280d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /Users/ineslopes/Documents/de_edit/.pipelines/lib/python3.9/site-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /Users/ineslopes/Documents/de_edit/.pipelines/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ineslopes/Documents/de_edit/.pipelines/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/01 23:18:25 WARN Utils: Your hostname, Iness-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.9 instead (on interface en0)\n",
            "24/12/01 23:18:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/12/01 23:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/12/01 23:18:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.master('local').appName('ETL Program').getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_api_data(endpoint, schema, spark):\n",
        "    response = requests.get(endpoint)\n",
        "    rdd = spark.sparkContext.parallelize(response.json())\n",
        "    df = spark.read.schema(schema).json(rdd)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                            StructField('block_id', StringType(), True),\n",
        "                            StructField('current_status', StringType(), True),\n",
        "                            StructField('id', StringType(), True),\n",
        "                            StructField('lat', FloatType(), True),\n",
        "                            StructField('line_id', StringType(), True),\n",
        "                            StructField('lon', FloatType(), True),\n",
        "                            StructField('pattern_id', StringType(), True),\n",
        "                            StructField('route_id', StringType(), True),\n",
        "                            StructField('schedule_relationship', StringType(), True),\n",
        "                            StructField('shift_id', StringType(), True),\n",
        "                            StructField('speed', FloatType(), True),\n",
        "                            StructField('stop_id', StringType(), True),\n",
        "                            StructField('timestamp', TimestampType(), True),\n",
        "                            StructField('trip_id', StringType(), True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "lines_schema = StructType([StructField('color', StringType(), True),\n",
        "                        StructField('facilities', ArrayType(StringType(), True), True),\n",
        "                        StructField('id', StringType(), True),\n",
        "                        StructField('localities',ArrayType(StringType(), True), True),\n",
        "                        StructField('long_name', StringType(), True),\n",
        "                        StructField('municipalities', ArrayType(StringType(), True), True),\n",
        "                        StructField('patterns', ArrayType(StringType(), True), True),\n",
        "                        StructField('routes', ArrayType(StringType(), True), True),\n",
        "                        StructField('short_name', StringType(), True), StructField('text_color', StringType(), True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "municipalities_schema = StructType([StructField('district_id', StringType(), True),\n",
        "                            StructField('district_name', StringType(), True),\n",
        "                            StructField('id', StringType(), True),\n",
        "                            StructField('name', StringType(), True),\n",
        "                            StructField('prefix', StringType(), True),\n",
        "                            StructField('region_id', StringType(), True),\n",
        "                            StructField('region_name', StringType(), True)\n",
        "                            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_vehicles = (\n",
        "    get_api_data(\"https://api.carrismetropolitana.pt/vehicles\", vehicle_schema, spark)\n",
        "                .withColumn(\"date\", expr(\"date(timestamp)\"))\n",
        "                )\n",
        "get_municipalities = get_api_data(\"https://api.carrismetropolitana.pt/municipalities\",municipalities_schema, spark)\n",
        "\n",
        "get_lines = get_api_data(\"https://api.carrismetropolitana.pt/lines\",lines_schema, spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Write Parquet\n",
        "get_vehicles.coalesce(1).write.mode(\"overwrite\").partitionBy(\"date\").format(\"parquet\").save(\"./content/lake/bronze/vehicles\")\n",
        "get_municipalities.coalesce(1).write.mode(\"overwrite\").format(\"parquet\").save(\"./content/lake/bronze/municipalities\")\n",
        "get_lines.coalesce(1).write.mode(\"overwrite\").format(\"parquet\").save(\"./content/lake/bronze/lines\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Transformations\n",
        "\n",
        "    - remove any corrupted record\n",
        "\n",
        "- Write data as PARQUET into the SILVER layer (/content/lake/silver)\n",
        "  - Partition \"vehicles\" by \"date\"(created in the ingestion)\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/silver/vehicles\n",
        "    - lines - path: /content/lake/silver/lines\n",
        "    - municipalities - path: /content/lake/silver/municipalities"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

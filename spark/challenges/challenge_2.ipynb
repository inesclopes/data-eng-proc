{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/challenges/challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ineslopes/Documents/de_edit/.pipelines/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "24/12/01 23:21:02 WARN Utils: Your hostname, Iness-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.9 instead (on interface en0)\n",
            "24/12/01 23:21:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/12/01 23:21:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Cleansing').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "lines_result=spark.read.format(\"parquet\").load(\"./content/lake/bronze/lines/*\")\n",
        "vehicles_result = spark.read.format(\"parquet\").load(\"./content/lake/bronze/vehicles/*\")\n",
        "municipalities_result = spark.read.format(\"parquet\").load(\"./content/lake/bronze/municipalities/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Apply basic transform\n",
        "df_v=(vehicles_result\n",
        "      .withColumnsRenamed({'lat':'latitude','lon':'longitude'})\n",
        "      .dropDuplicates()\n",
        "      .dropna(subset='CURRENT_STATUS')\n",
        "      .withColumn(\"date\", expr(\"date(timestamp)\"))\n",
        "      )\n",
        "\n",
        "df_l=(lines_result\n",
        "      .dropDuplicates()\n",
        "      .dropna(subset='LONG_NAME')\n",
        "      )\n",
        "\n",
        "df_m=(municipalities_result\n",
        "      .dropDuplicates()\n",
        "      .dropna(how='any',subset=['DISTRICT_NAME','NAME'])\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "(df_v.coalesce(1)\n",
        " .write.mode(\"overwrite\")\n",
        " .partitionBy(\"date\")\n",
        " .format(\"parquet\")\n",
        " .save(\"./content/lake/silver/vehicles\")\n",
        ")\n",
        " \n",
        "(df_l.coalesce(1)\n",
        " .write.mode(\"overwrite\")\n",
        " .format(\"parquet\")\n",
        " .save(\"./content/lake/silver/lines\")\n",
        ")\n",
        " \n",
        "(df_m.coalesce(1)\n",
        " .write.mode(\"overwrite\")\n",
        " .format(\"parquet\")\n",
        " .save(\"./content/lake/silver/municipalities\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CHALLENGE 2\n",
        "##  Implement CLEANSING process\n",
        "- Set up path in the \"lake\"\n",
        "  - !mkdir -p /content/lake/silver\n",
        "\n",
        "- Read data from BRONZE layer as PARQUET:\n",
        "    - vehicles - path: /content/lake/bronze/vehicles\n",
        "    - lines - path: /content/lake/bronze/lines\n",
        "    - municipalities - path: /content/lake/bronze/municipalities\n",
        "\n",
        "- Transformations\n",
        "  - vehicles\n",
        "    - rename \"lat\" and \"lon\" to \"latitude\" and \"longitude\" respectively\n",
        "    - remove possible duplicates\n",
        "    - remove rows when the column CURRENT_STATUS is null\n",
        "    - remove any corrupted record\n",
        "  - lines\n",
        "    - remove duplicates\n",
        "    - remove rows when the column LONG_NAME is null\n",
        "    - remove any corrupted record\n",
        "  - municipalities\n",
        "    - remove duplicates\n",
        "    - remove rows when the columns NAME or DISTRICT_NAME are null\n",
        "    - remove any corrupted record\n",
        "\n",
        "- Write data as PARQUET into the SILVER layer (/content/lake/silver)\n",
        "  - Partition \"vehicles\" by \"date\"(created in the ingestion)\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/silver/vehicles\n",
        "    - lines - path: /content/lake/silver/lines\n",
        "    - municipalities - path: /content/lake/silver/municipalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "c410e46c-4a50-43aa-926f-d0417c6280d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
